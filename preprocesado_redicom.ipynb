{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import pydicom as pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# \n",
    "#.........FUNCIONES PREPROCESADO.............\n",
    "#############################################\n",
    "\n",
    "#FUNCIONES NECESARIAS PARA DICOM / PREPROCESADO DE LA IMAGEN (SON 5 PASOS):\n",
    "#1. CARGAR LA IMAGEN:\n",
    "def load_scan(path): #will load all DICOM images from a folder into a list for manipulation.\n",
    "    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "        \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "    return slices\n",
    "\n",
    "#2. CONVERTIR EL RAW DATA EN UNIDADES HOUNSFIELD, PASO PREVIO A NORMALIZAR. \n",
    "def get_pixels_hu(slices): #The voxel values in the images are raw. get_pixels_hu converts raw values into HU. The transformation is linear. Therefore, so long as you have a slope and an intercept, you can rescale a voxel value to HU.\n",
    "                           #Both the rescale intercept and rescale slope are stored in the DICOM header at the time of image acquisition (these values are scanner-dependent, so you will need external information).\n",
    "    \n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    hist(np.array(image))\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "    \n",
    "    \n",
    "    # Set outside-of-scan pixels to 0. \n",
    "    # The intercept is usually -1024, so air is approximately 0 (sin unidad, es -1000 en UH).\n",
    "    image[image == -2000] = 0\n",
    "        # Convert to Hounsfield units (HU)\n",
    "    for slice_number in range(len(slices)):\n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        \n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "            \n",
    "        image[slice_number] += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "#3. NORMALIZAR:\n",
    "def normalize(image):\n",
    "    MIN_BOUND=15\n",
    "    MAX_BOUND=100\n",
    "    \n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    \n",
    "    image[image>1] = 0. #quitar información\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "#4. RESAMPLING (para que todas tengas la misma \"resolución\"): \n",
    "def resample(image, scan, new_spacing=[5,5,5]):\n",
    "    # Determine current pixel spacing\n",
    "    pi=scan[0].PixelSpacing\n",
    "    pi=pi[0]\n",
    "    spacing = np.array(scan[0].SliceThickness + pi,np.float32)\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    import scipy\n",
    "    from  scipy import ndimage\n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest')\n",
    "    \n",
    "    return image, new_spacing\n",
    "\n",
    "\n",
    "#5. \"PREPROCESADO\" o RESAMPLING_AVANZADO (es decir, que todos los pacientes tengan el mismo número de slices): \n",
    "def preprocesado(pixels, slices, dir_final): #todos los pacientes tienen que tener el mismo númro de cortes. \n",
    "    #HAY QUE HACER UN RESIZE!!!!\n",
    "    \n",
    "    pix_resampled, spacing = resample(pixels, slices, [5,5,5])\n",
    "    \n",
    "    pixels_n=[]\n",
    "    for s in pix_resampled:\n",
    "        #pixels1=s.pixel_array\n",
    "        pixels1=s\n",
    "        pixels1=normalize(pixels1) #SE NORMALIZAN AQUÍ\n",
    "        pixels1=list(pixels1)\n",
    "        pixels_n.append(pixels1)\n",
    "    pixels_n=np.array(pixels_n)\n",
    "    \n",
    "    '''n,m,l=pixels_n.shape    \n",
    "    pixels_n=list(pixels_n)\n",
    "    \n",
    "    \n",
    "    matriz_float = np.zeros((m, l))\n",
    "    matriz_float = np.int16(matriz_float)\n",
    "    for ind in range(n,41):\n",
    "        pixels_n.append(matriz_float)\n",
    "    pixels_n=np.array(pixels_n)'''\n",
    "    np.save(dir_final, pixels_n)\n",
    "    \n",
    "    return pixels_n\n",
    "\n",
    "\n",
    "#6. ZERO CENTERING (¿queremos hacerlo?)--> https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial\n",
    "def zero_center(image, PIXEL_MEAN=0.25):\n",
    "    image = image - PIXEL_MEAN\n",
    "    return image\n",
    "\n",
    "\n",
    "# VISUALIZAR CORTES\n",
    "def sample_stack(stack, rows=5, cols=5, start_with=0, show_every=1):\n",
    "    fig,ax = plt.subplots(rows,cols,figsize=[12,12])\n",
    "    for i in range(rows*cols):\n",
    "        ind = start_with + i*show_every\n",
    "        ax[int(i/rows),int(i % rows)].set_title('slice %d' % ind)\n",
    "        ax[int(i/rows),int(i % rows)].imshow(stack[ind],cmap='gray')\n",
    "        ax[int(i/rows),int(i % rows)].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# VISUALIZAR CORTES\n",
    "def hist(stack):\n",
    "    fig= plt.hist(stack.flatten(), bins=80)\n",
    "    plt.xlabel(\"Hounsfield Units (HU)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "#CREAR LOS CSV\n",
    "def crear_csv(train_list, label_list, name_csv, data=[]):\n",
    "    if name_csv is not 'TRAIN':\n",
    "        train_list=np.transpose(train_list)\n",
    "        label_list=np.transpose(label_list)\n",
    "\n",
    "        data = {'name': train_list,\n",
    "                'label': train_list}\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "    df.to_csv('/srv/data/HIC/datos/csv/DICOM'+name_csv+'.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": {},
   "source": [
    "###############################################################################################\n",
    "#.........CREAMOS CSV Y HACEMOS EL PREPROCESADO DE LAS IMÁGENES PARA HIC Y CONTROL.............\n",
    "############################################################################################### \n",
    "    \n",
    "    \n",
    "#CREAMOS EL EXCEL CON LOS PATHS Y LAS ETIQUETAS: \n",
    "#CASOS:\n",
    "train_list = []\n",
    "label_list = []\n",
    "\n",
    "for dirpath, dirnames, fileList in os.walk('/srv/data/HIC/datos/HIC/DATASETS_HIC/'):\n",
    "    for direc in dirnames:\n",
    "        if direc is '.ipynb_checkpoints':\n",
    "            print('es .ipynb_checkpoints')\n",
    "        else:\n",
    "            print('HIC_',direc)\n",
    "            dir_final='/srv/data/HIC/datos/3d_dicom/HIC_'+str(direc)+'.npy'\n",
    "            for dirpath1, dirnames1, fileList1 in os.walk('/srv/data/HIC/datos/HIC/DATASETS_HIC/'+str(direc)):\n",
    "\n",
    "                slices = load_scan(dirpath1)\n",
    "                print('[INFO] ANTES DE LA CONVERSIÓN')\n",
    "                pixels = get_pixels_hu(slices)\n",
    "                \n",
    "                print('[INFO] DESPUÉS DE LA CONVERSIÓN')\n",
    "                hist(np.array(pixels))\n",
    "                pixels_n = preprocesado(pixels, slices, dir_final) \n",
    "                #pix_resampled, spacing = resample(pixels, slices, [5,5,5])\n",
    "                \n",
    "                print('[INFO] NORMALIZADO')\n",
    "                hist(pixels_n)\n",
    "                sample_stack(pixels_n, rows=4, cols=4, start_with=0, show_every=1)\n",
    "                \n",
    "                #resize dentro del pre procesado- no se puede usar cv2 entonces hay que buscar otra libreria            \n",
    "                #lista para csv\n",
    "                train_list.append(dir_final)\n",
    "                label_list.append(1)\n",
    "train_list_HIC=train_list\n",
    "label_list_HIC=label_list     \n",
    "\n",
    "#Crear CSV train\n",
    "crear_csv(train_list, label_list, 'TRAIN_HIC')\n",
    "\n",
    "\n",
    "#CONTROLES:\n",
    "train_list = []\n",
    "label_list = []\n",
    "\n",
    "dirnames= !ls '/srv/data/HIC/datos/CONTROL/DICOM_NORMALES/CONTROLES_DICOM_CODIFICADOS/FILESET/'\n",
    "\n",
    "for direc in dirnames:\n",
    "    print('CONTROL_',direc)\n",
    "    dir_final='/srv/data/HIC/datos/3d_dicom/CONTROL_'+str(direc)+'.npy'\n",
    "    for dirpath1, dirnames1, fileList1 in os.walk('/srv/data/HIC/datos/CONTROL/DICOM_NORMALES/CONTROLES_DICOM_CODIFICADOS/FILESET/'+str(direc)+'/0/0/'):\n",
    "        #print(dirpath1)\n",
    "        #ORDENAR LOS CORTES\n",
    "        slices = load_scan(dirpath1)\n",
    "        print('[INFO] ANTES DE LA CONVERSIÓN')\n",
    "        pixels = get_pixels_hu(slices)\n",
    "                \n",
    "        print('[INFO] DESPUÉS DE LA CONVERSIÓN')\n",
    "        hist(np.array(pixels))\n",
    "        \n",
    "        pixels_n = preprocesado(pixels, slices, dir_final) \n",
    "        print('[INFO] NORMALIZADO')\n",
    "        hist(pixels_n)\n",
    "        sample_stack(pixels_n, rows=4, cols=4, start_with=0, show_every=1)\n",
    "        \n",
    "        #LISTA PARA HACER EL CSV\n",
    "        train_list.append(dir_final)\n",
    "        label_list.append(0)    \n",
    "\n",
    "train_list_CONTROL=train_list\n",
    "label_list_CONTROL=label_list\n",
    "crear_csv(train_list, label_list, 'TRAIN_CONTROL')\n",
    "\n",
    "train_list_CONTROL=np.transpose(train_list_CONTROL)\n",
    "label_list_CONTROL=np.transpose(label_list_CONTROL)\n",
    "train_list_HIC=np.transpose(train_list_HIC)\n",
    "label_list_HIC=np.transpose(label_list_HIC)\n",
    "\n",
    "names=np.concatenate((train_list_CONTROL, train_list_HIC))\n",
    "labels=np.concatenate((label_list_CONTROL, label_list_HIC))\n",
    "\n",
    "#TRAIN\n",
    "data = {'name': names,'label': labels}\n",
    "crear_csv(names, labels, '_', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VISUALIZACIÓN_FINAL_CASO(HIC):\n",
    "imgs_to_process = np.load('/srv/data/HIC/datos/3d_dicom/CONTROL_47.npy', allow_pickle=True)\n",
    "plt.hist(imgs_to_process.flatten(), bins=80)\n",
    "plt.xlabel(\"Hounsfield Units (HU)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "sample_stack(imgs_to_process, rows=4, cols=4, start_with=0, show_every=1)\n",
    "\n",
    "#VISUALIZACIÓN_FINAL_CONTROL:\n",
    "imgs_to_process = np.load('/srv/data/HIC/datos/3d_dicom/HIC_131.npy',allow_pickle=True)\n",
    "plt.hist(imgs_to_process.flatten(), bins=80)\n",
    "plt.xlabel(\"Hounsfield Units (HU)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "sample_stack(imgs_to_process, rows=4, cols=4, start_with=0, show_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split=0.20\n",
    "SPLIT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAR KFOLDS\n",
    "\n",
    "# read data\n",
    "# Assuming it has two cols:\n",
    "# name: path to each image with its extension\n",
    "# label: labels (here it is 0s and 1s) -> binary classification\n",
    "df = pd.read_csv('/srv/data/HIC/datos/csv/DICOM_.csv')\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "df = df.astype(str)\n",
    "\n",
    "# split for testing\n",
    "train_df, test_df = train_test_split(df, test_size=test_split)\n",
    "\n",
    "print('[INFO] TRAIN CSV ',train_df.shape)\n",
    "print('[INFO] TEST CSV ',test_df.shape)\n",
    "train_df.to_csv('/srv/data/HIC/datos/csv/TRAIN.csv')\n",
    "test_df.to_csv('/srv/data/HIC/datos/csv/TEST.csv')\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "    \n",
    "Xtrain = train_df['name']\n",
    "ytrain = train_df['label']\n",
    "\n",
    "print(\"KFold, shuffle=False (default)\")\n",
    "kf = KFold(n_splits=SPLIT, random_state=True, shuffle=True)\n",
    "i=1\n",
    "X_train=[]\n",
    "X_val=[]\n",
    "y_train=[]\n",
    "y_val=[]\n",
    "\n",
    "for train_index, test_index in kf.split(Xtrain, ytrain):\n",
    "    #print('TRAIN_'+str(i)+' :',train_index)\n",
    "    #print('TEST_'+str(i)+' :', test_index)\n",
    "    train_index=list(train_index)\n",
    "    test_index=list(test_index)\n",
    "\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    if(i is 1):\n",
    "        X_train1 , X_val1 = Xtrain[train_index], Xtrain[test_index]\n",
    "        y_train1, y_val1 = ytrain[train_index], ytrain[test_index]\n",
    "        \n",
    "        data = {'name': X_train1,\n",
    "        'label': y_train1}\n",
    "        dftrain1 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dftrain1.to_csv('/srv/data/HIC/datos/csv/TRAIN'+str(i)+'.csv')\n",
    "        \n",
    "        data = {'name': X_val1,\n",
    "        'label': y_val1}\n",
    "        dfval1 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dfval1.to_csv('/srv/data/HIC/datos/csv/VAL'+str(i)+'.csv')\n",
    "        \n",
    "        i=i+1\n",
    "    elif(i is 2):\n",
    "        X_train2 , X_val2 = Xtrain[train_index], Xtrain[test_index]\n",
    "        y_train2, y_val2 = ytrain[train_index], ytrain[test_index]\n",
    "        \n",
    "        data = {'name': X_train2,\n",
    "        'label': y_train2}\n",
    "        dftrain2 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dftrain2.to_csv('/srv/data/HIC/datos/csv/TRAIN'+str(i)+'.csv')\n",
    "        \n",
    "        data = {'name': X_val2,\n",
    "        'label': y_val2}\n",
    "        dfval2 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dfval2.to_csv('/srv/data/HIC/datos/csv/VAL'+str(i)+'.csv')\n",
    "        \n",
    "        i=i+1\n",
    "    elif(i is 3):\n",
    "        X_train3 , X_val3 = Xtrain[train_index], Xtrain[test_index]\n",
    "        y_train3, y_val3 = ytrain[train_index], ytrain[test_index]\n",
    "        \n",
    "        data = {'name': X_train3,\n",
    "        'label': y_train3}\n",
    "        dftrain3 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dftrain3.to_csv('/srv/data/HIC/datos/csv/TRAIN'+str(i)+'.csv')\n",
    "        \n",
    "        data = {'name': X_val3,\n",
    "        'label': y_val3}\n",
    "        dfval3 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dfval3.to_csv('/srv/data/HIC/datos/csv/VAL'+str(i)+'.csv')\n",
    "        \n",
    "        i=i+1\n",
    "    elif(i is 4):\n",
    "        X_train4 , X_val4 = Xtrain[train_index], Xtrain[test_index]\n",
    "        y_train4, y_val4 = ytrain[train_index], ytrain[test_index]\n",
    "        \n",
    "        data = {'name': X_train4,\n",
    "        'label': y_train4}\n",
    "        dftrain4 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dftrain4.to_csv('/srv/data/HIC/datos/csv/TRAIN'+str(i)+'.csv')\n",
    "        \n",
    "        data = {'name': X_val4,\n",
    "        'label': y_val4}\n",
    "        dfval4 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dfval4.to_csv('/srv/data/HIC/datos/csv/VAL'+str(i)+'.csv')\n",
    "        \n",
    "        i=i+1\n",
    "    elif(i is 5):\n",
    "        X_train5 , X_val5 = Xtrain[train_index], Xtrain[test_index]\n",
    "        y_train5, y_val5 = ytrain[train_index], ytrain[test_index]\n",
    "        \n",
    "        data = {'name': X_train5,\n",
    "        'label': y_train5}\n",
    "        dftrain5 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dftrain5.to_csv('/srv/data/HIC/datos/csv/TRAIN'+str(i)+'.csv')\n",
    "        \n",
    "        data = {'name': X_val5,\n",
    "        'label': y_val5}\n",
    "        dfval5 = pd.DataFrame(data, columns = ['name', 'label']) \n",
    "        dfval5.to_csv('/srv/data/HIC/datos/csv/VAL'+str(i)+'.csv')       \n",
    "        i=i+1\n",
    "        \n",
    "#REVISAR KFOLDS HECHOS Y TAMAÑO   \n",
    "print('\\nTRAIN 1',dftrain1.shape)\n",
    "print('VAL 1', dfval1.shape)\n",
    "\n",
    "print('TRAIN 2',dftrain2.shape)\n",
    "print('VAL 2',dfval2.shape)\n",
    "\n",
    "print('TRAIN 3',dftrain3.shape)\n",
    "print('VAL 3',dfval3.shape)\n",
    "\n",
    "print('TRAIN 4',dftrain4.shape)\n",
    "print('VAL 4',dfval4.shape)\n",
    "\n",
    "print('TRAIN 5',dftrain5.shape)\n",
    "print('VAL 5',dfval5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
