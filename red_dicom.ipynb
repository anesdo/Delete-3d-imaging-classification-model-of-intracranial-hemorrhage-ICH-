{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%pip install tensorflow==2.2.0\n",
    "%pip install tensorflow-gpu==2.2.0 '''\n",
    "\n",
    "'''%pip install --upgrade pip\n",
    "%pip install --upgrade gast\n",
    "%pip install --upgrade tensorflow-gpu\n",
    "%pip install --upgrade tensorboard'''\n",
    "#%pip install gast==0.3.3\n",
    "#%pip install tensorflow==2.2.0\n",
    "#%pip install opencv-python-headless\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import pydicom as pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution3D, MaxPooling3D\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta, SGD\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from scipy import ndimage   #Multidimensional image processing\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# \n",
    "#.............ARQUITECTURA...................\n",
    "#############################################\n",
    "def get_model(width=128, height=128, depth=45, top='GAP'):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if top is 'GMP':\n",
    "        x = layers.GlobalMaxPooling3D()(x)\n",
    "        x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_anna(width=128, height=128, depth=45, top='GAP'):\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "            #initializer = tf.kehist(stack)as.initializers.GlorotNormal()\n",
    "    \n",
    "    \n",
    "    x = layers.Conv3D(filters=16, kernel_size=3,  activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=32, kernel_size=3,  activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=3,  activation=\"relu\")(x)\n",
    "    \n",
    "    \n",
    "    if top is 'GMP':\n",
    "        x = layers.GlobalMaxPooling3D()(x)\n",
    "        '''x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)'''\n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        '''x = layers.Dense(units=512,  activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)'''\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "############################################# \n",
    "#.............OPTIMIZADOR....................\n",
    "#############################################\n",
    "def _get_optimizer(optimizer_model, lr_n):\n",
    "    print(\"[INFO] OPTIMIZADOR: \", optimizer_model)\n",
    "    # Initiate the optimizer.\n",
    "    if optimizer_model == 'Adam':\n",
    "        opt = Adam(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'Adagrad':\n",
    "        opt = Adagrad(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'Adadelta':\n",
    "        opt = Adadelta(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'SGD':\n",
    "        opt = SGD(lr_n)\n",
    "    else:\n",
    "        raise ValueError('Optimizer not implemented yet!')\n",
    "    return opt\n",
    "\n",
    "############################################# \n",
    "#.........FUNCIÓN ENTRENAMIENTO..............\n",
    "#############################################\n",
    "def train_classifier(iepoch, ibatch, ifold, optimizer_model, lr_schedule, train_dataset, label_train, validation_dataset, label_val, itop):\n",
    "    #model = get_model(width=128, height=128, depth=45, top=itop)\n",
    "    model = get_model_anna(width=128, height=128, depth=45, top=itop)\n",
    "    model.summary()\n",
    "    \n",
    "    print('[INFO] COMPILAR')\n",
    "    # Compile model.\n",
    "    opt=_get_optimizer(optimizer_model, lr_schedule)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=opt,\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "    \n",
    "    # Define callbacks.\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        \"/srv/data/HIC/code/resultados/DICOM/DICOM_top\"+str(itop)+\"_batch\"+str(ibatch)+\"_opt\"+str(optimizer_model)+\"_lr\"+str(lr_schedule)+\"_kfold\"+str(ifold)+\".h5\", save_best_only=True\n",
    "    )\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    print(train_dataset.shape)\n",
    "    print(validation_dataset.shape)\n",
    "    \n",
    "    '''for ind in range(0,15):\n",
    "        sample_stack(train_dataset[ind,:,:,:] , rows=6, cols=6, start_with=0, show_every=1)\n",
    "        hist(train_dataset[ind,:,:,:])'''\n",
    "        \n",
    "    print('[INFO] ENTRENAMIENTO')\n",
    "    # Train the model, doing validation at the end of each epoch\n",
    "    model.fit(\n",
    "        x=train_dataset,\n",
    "        y=label_train,\n",
    "        epochs=iepoch,\n",
    "        batch_size=ibatch,\n",
    "        validation_data=(validation_dataset,label_val),\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "    )    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "    ax = ax.ravel()\n",
    "    for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "        ax[i].plot(model.history.history[metric])\n",
    "        ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "        ax[i].set_title(\"Model {}\".format(metric))\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].set_ylabel(metric)\n",
    "        ax[i].legend([\"train\", \"val\"])\n",
    "        plt.savefig(\"/srv/data/HIC/code/resultados/DICOM/LOSS_ACC_top\"+str(itop)+\"_batch\"+str(ibatch)+\"_opt\"+str(optimizer_model)+\"_lr\"+str(lr_schedule)+\"_kfold\"+str(ifold)+\".png\")\n",
    "    \n",
    "    # Clean GPU memory\n",
    "    del model\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# \n",
    "#...............PREPROCESADO.................\n",
    "#############################################\n",
    "\n",
    "def read_file(filepath):   #Aquí solo define la función. \n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    #print(filepath)\n",
    "    much_data = np.load(filepath)\n",
    "    \n",
    "    #print(much_data.shape)\n",
    "    \n",
    "    return much_data\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 45\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def resize_volume_black(img):    \n",
    "    n,m,l=img.shape\n",
    "    #print('img',img.shape)\n",
    "    img_p=list(img)\n",
    "\n",
    "    matriz_float = np.zeros((m,l))\n",
    "    #print('matriz_float',matriz_float.shape)\n",
    "    \n",
    "    matriz_float = np.float32(matriz_float)\n",
    "    for ind in range(n,45):\n",
    "        img_p.append(matriz_float)\n",
    "    img_p=np.array(img_p)\n",
    "    #print('img_nuevo',img_p.shape)\n",
    "    \n",
    "    \n",
    "    n,m,l=img_p.shape\n",
    "    img_data=np.zeros([128,128,n])\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        img_data[:,:,i]=cv2.resize(img_p[i,:,:],(128,128))\n",
    "    '''hist(img_data)\n",
    "    sample_stack(img_data, rows=6, cols=6, start_with=0, show_every=1)'''\n",
    "    #print(img_data.shape)\n",
    "    \n",
    "    return img_data\n",
    "\n",
    "#COMPENDIO DE LAS FUNCIONES DEFINIDAS ARRIBA:\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    #print(path)\n",
    "    # Read scan\n",
    "    volume = read_file(path)    #no importan tanto el nombre (path o filepath) sino el ORDEN!!!\n",
    "    '''hist(volume)\n",
    "    sample_stack(volume, rows=4, cols=4, start_with=0, show_every=1)'''\n",
    "    #print('antes',volume.shape)\n",
    "    \n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume_black(volume)\n",
    "    #print('despues',volume.shape)\n",
    "    '''hist(volume)\n",
    "    sample_stack(volume, rows=6, cols=6, start_with=0, show_every=1)'''\n",
    "   \n",
    "    return volume\n",
    "# VISUALIZAR CORTES\n",
    "def sample_stack(stack, rows=5, cols=5, start_with=0, show_every=1):\n",
    "    fig,ax = plt.subplots(rows,cols,figsize=[12,12])\n",
    "    for i in range(rows*cols):\n",
    "        ind = start_with + i*show_every\n",
    "        ax[int(i/rows),int(i % rows)].set_title('slice %d' % ind)\n",
    "        ax[int(i/rows),int(i % rows)].imshow(stack[:,:,ind],cmap='gray')\n",
    "        ax[int(i/rows),int(i % rows)].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# VISUALIZAR CORTES\n",
    "def hist(stack):\n",
    "    fig= plt.hist(stack.flatten(), bins=80)\n",
    "    plt.xlabel(\"Hounsfield Units (HU)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "#....INICIALIZAR PARÁMETROS PARA EL ENTRENAMIENTO..............\n",
    "###############################################################\n",
    "Kfold=[0,1,2,3,4,5]\n",
    "batchs=[2] \n",
    "epochs = 10\n",
    "lr=[0.35,0.3]\n",
    "optimizer_model=['Adadelta'] # 'SGD', 'Adam', 'SGD'\n",
    "top=['GMP'] #,'GAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] FOLD 0\n",
      "\n",
      "[INFO] LEYENDO CSV\n",
      "Number of samples in train and validation are 165 and 42.\n",
      "[INFO] BATCH 2\n",
      "[INFO] FOLD 0\n",
      "[INFO] OPT Adadelta\n",
      "[INFO] LR 0.35\n",
      "[INFO] topmodel GMP\n",
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 45, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 126, 126, 43, 16)  448       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 63, 63, 21, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 61, 61, 19, 32)    13856     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 30, 30, 9, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 28, 28, 7, 64)     55360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling3d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 69,729\n",
      "Trainable params: 69,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[INFO] COMPILAR\n",
      "[INFO] OPTIMIZADOR:  Adadelta\n",
      "(165, 128, 128, 45, 1)\n",
      "(42, 128, 128, 45, 1)\n",
      "[INFO] ENTRENAMIENTO\n",
      "Train on 165 samples, validate on 42 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "for ibatch in batchs:\n",
    "    for ifold in Kfold:\n",
    "        print('[INFO] FOLD',str(ifold))\n",
    "        print('\\n[INFO] LEYENDO CSV')\n",
    "        if ifold is not 0:\n",
    "            CSV_TRAIN = pd.read_csv('/srv/data/HIC/datos/csv/TRAIN'+str(ifold)+'.csv',sep=',')\n",
    "            CSV_VAL = pd.read_csv('/srv/data/HIC/datos/csv/VAL'+str(ifold)+'.csv',sep=',')\n",
    "        else:\n",
    "            CSV_TRAIN = pd.read_csv('/srv/data/HIC/datos/csv/TRAIN.csv',sep=',')\n",
    "            CSV_VAL = pd.read_csv('/srv/data/HIC/datos/csv/TEST.csv',sep=',')\n",
    "        train_dataset=[]\n",
    "        for i in CSV_TRAIN['name']:\n",
    "            #print(i)\n",
    "            much_data= process_scan(i)\n",
    "            #print(much_data.shape)\n",
    "            train_dataset.append(much_data)         \n",
    "        train_dataset=np.array(train_dataset)\n",
    "        x_train = np.expand_dims(train_dataset, axis=4)\n",
    "        y_train=CSV_TRAIN['label']\n",
    "        \n",
    "        validation_dataset=[]\n",
    "        for i in CSV_VAL['name']:\n",
    "            much_data= process_scan(i)\n",
    "            validation_dataset.append(much_data)\n",
    "            validation_dataset=np.array(validation_dataset)\n",
    "            validation_dataset = validation_dataset.astype(np.float32)\n",
    "            validation_dataset=list(validation_dataset)\n",
    "\n",
    "        validation_dataset=np.array(validation_dataset)\n",
    "        x_val= np.expand_dims(validation_dataset, axis=4)\n",
    "        y_val=CSV_VAL['label']\n",
    "        \n",
    "        x_train = np.asarray(x_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        x_val = np.asarray(x_val)\n",
    "        y_val = np.asarray(y_val)\n",
    "        print(\n",
    "            \"Number of samples in train and validation are %d and %d.\" \n",
    "            % (x_train.shape[0], x_val.shape[0])\n",
    "        )\n",
    "        '''# Define data loaders.\n",
    "        train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "        print(train_loader)\n",
    "    \n",
    "        batch_size = ibatch\n",
    "        print('[INFO] BATCH',str(ibatch))\n",
    "        \n",
    "        # Augment the on the fly during training.\n",
    "        train_dataset = (\n",
    "            train_loader.shuffle(len(x_train))\n",
    "            .map(train_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "        # Only rescale.\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(x_val))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )'''\n",
    "        for ilr in lr:\n",
    "            '''initial_learning_rate = ilr_p\n",
    "            lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "            )\n",
    "            ilr=lr_schedule'''\n",
    "            for iopt in optimizer_model:\n",
    "                for itop in top:\n",
    "                    print('[INFO] BATCH',str(ibatch))           \n",
    "                    print('[INFO] FOLD',str(ifold))\n",
    "                    print('[INFO] OPT',str(iopt))\n",
    "                    print('[INFO] LR',str(ilr))\n",
    "                    print('[INFO] topmodel',str(itop))\n",
    "                    \n",
    "                    '''print('\\n[INFO] LEYENDO CSV')\n",
    "                    CSV_TRAIN = pd.read_csv('/srv/data/HIC/datos/csv/TRAIN'+str(ifold)+'.csv',sep=',')\n",
    "                    CSV_VAL = pd.read_csv('/srv/data/HIC/datos/csv/VAL'+str(ifold)+'.csv',sep=',')\n",
    "\n",
    "                    train_dataset=[]\n",
    "                    for i in CSV_TRAIN['name']:\n",
    "                        much_data = np.load(i)\n",
    "                        much_data.resize((45, 128, 128))\n",
    "                        much_data=much_data.reshape(much_data.shape[1], much_data.shape[2], much_data.shape[0])\n",
    "\n",
    "                        #print(much_data.shape)\n",
    "\n",
    "                        train_dataset.append(much_data)\n",
    "                        train_dataset=np.array(train_dataset)\n",
    "                        train_dataset = train_dataset.astype(np.int16)\n",
    "                        train_dataset=list(train_dataset)\n",
    "                    train_dataset=np.array(train_dataset)\n",
    "                    train_dataset = np.expand_dims(train_dataset, axis=4)\n",
    "                    label_train=CSV_TRAIN['label']\n",
    "\n",
    "                    validation_dataset=[]\n",
    "                    for i in CSV_VAL['name']:\n",
    "                        much_data = np.load(i)\n",
    "                        much_data.resize((45, 128, 128))\n",
    "                        much_data=much_data.reshape(much_data.shape[1], much_data.shape[2], much_data.shape[0])\n",
    "\n",
    "                        validation_dataset.append(much_data)\n",
    "                        validation_dataset=np.array(validation_dataset)\n",
    "                        validation_dataset = validation_dataset.astype(np.int16)\n",
    "                        validation_dataset=list(validation_dataset)\n",
    "\n",
    "                    validation_dataset=np.array(validation_dataset)\n",
    "                    validation_dataset= np.expand_dims(validation_dataset, axis=4)\n",
    "                    label_val=CSV_VAL['label']'''\n",
    "\n",
    "                    #Entrenar\n",
    "                    train_classifier(epochs, ibatch, ifold, iopt, ilr, x_train, y_train, x_val, y_val, itop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tabulate import tabulate\n",
    "def plot_confusion_matrix(y_true, y_pred, numfold, classes, normalize=False, title=None, cmap=plt.cm.Blues, dir_out='/srv/data/HIC/code/resultados/DICOM/'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = [classes['0'], classes['1'], classes['2'], classes['3']]\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax.figure.savefig(dir_out + '/kfold' + str(numfold))\n",
    "    plt.close()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def report_classification_results(y_true, y_pred, classes, numfold, paths='/srv/data/HIC/code/resultados/DICOM/'):\n",
    "    # Extract confusion matrix\n",
    "    conf_mat = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute metrics\n",
    "    headers = []\n",
    "    g_TP, g_TN, g_FN, g_FP = [], [], [], []\n",
    "    ss, ee, pp, nn, ff, aa = [], [], [], [], [], []\n",
    "    S, E, PPV, NPV, F, ACC = ['Sensitivity'], ['Specificity'], ['PPV (precision)'], ['NPV'], ['F1-score'], ['Accuracy']\n",
    "    for i in range(len(classes)):\n",
    "        headers.append(classes[str(i)])\n",
    "\n",
    "        # Extract indicators per class\n",
    "        TP = conf_mat[i, i]\n",
    "        TN = sum(sum(conf_mat)) - sum(conf_mat[i, :]) - sum(conf_mat[:, i]) + TP\n",
    "        FN = sum(conf_mat[i, :]) - conf_mat[i, i]\n",
    "        FP = sum(conf_mat[:, i]) - conf_mat[i, i]\n",
    "\n",
    "        # Extract metrics per class\n",
    "        sen = TP / (TP + FN)\n",
    "        spe = TN / (TN + FP)\n",
    "        ppv = TP / (TP + FP)\n",
    "        npv = TN / (TN + FN)\n",
    "        f1_s = 2 * ppv * sen / (ppv + sen)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        # Create table for printing\n",
    "        S.append(str(round(sen, 4)))\n",
    "        E.append(str(round(spe, 4)))\n",
    "        PPV.append(str(round(ppv, 4)))\n",
    "        NPV.append(str(round(npv, 4)))\n",
    "        F.append(str(round(f1_s, 4)))\n",
    "        ACC.append(str(round(acc, 4)))\n",
    "\n",
    "        # Global indicators\n",
    "        g_TP.append(TP), g_TN.append(TN), g_FP.append(FP), g_FN.append(FN)\n",
    "        # Global metrics\n",
    "        ss.append(sen), ee.append(spe), pp.append(ppv), nn.append(npv), ff.append(f1_s), aa.append(acc)\n",
    "\n",
    "    # Extract micro values\n",
    "    micro_S = round(sum(g_TP) / (sum(g_TP) + sum(g_FN)), 4)\n",
    "    micro_E = round(sum(g_TN) / (sum(g_TN) + sum(g_FP)), 4)\n",
    "    micro_PPV = round(sum(g_TP) / (sum(g_TP) + sum(g_FP)), 4)\n",
    "    micro_NPV = round(sum(g_TN) / (sum(g_TN) + sum(g_FN)), 4)\n",
    "    micro_F1 = round(2 * micro_S * micro_PPV / (micro_S + micro_PPV), 4)\n",
    "    micro_Acc = round((sum(g_TP) + sum(g_TN)) / (sum(g_TP) + sum(g_TN) + sum(g_FP) + sum(g_FN)), 4)\n",
    "    # Extract macro values\n",
    "    macro_S = round(sum(ss) / len(ss), 4)\n",
    "    macro_E = round(sum(ee) / len(ee), 4)\n",
    "    macro_PPV = round(sum(pp) / len(pp), 4)\n",
    "    macro_NPV = round(sum(nn) / len(nn), 4)\n",
    "    macro_F1 = round(sum(ff) / len(ff), 4)\n",
    "    macro_Acc = round(sum(aa) / len(aa), 4)\n",
    "\n",
    "    # Construct the table\n",
    "    S.append('---'), E.append('---'), PPV.append('---'), NPV.append('---'), F.append('---'), ACC.append('---')\n",
    "    S.append(micro_S), E.append(micro_E), PPV.append(micro_PPV), NPV.append(micro_NPV), F.append(micro_F1), ACC.append(\n",
    "        micro_Acc)\n",
    "    S.append(macro_S), E.append(macro_E), PPV.append(macro_PPV), NPV.append(macro_NPV), F.append(macro_F1), ACC.append(\n",
    "        macro_Acc)\n",
    "\n",
    "    # Define table\n",
    "    my_data = [tuple(S), tuple(E), tuple(PPV), tuple(NPV), tuple(F), tuple(ACC)]\n",
    "    df = pd.DataFrame(my_data)\n",
    "    df.to_csv(paths + '/metrics_kfold' + str(numfold) + '.csv')\n",
    "\n",
    "    # MODEL metrics\n",
    "    if len(classes) > 2:\n",
    "        auc = metrics.roc_auc_score(y_true=to_categorical(y_true, num_classes=len(classes)),\n",
    "                                    y_score=to_categorical(y_pred, num_classes=len(classes)), multi_class='ovr')\n",
    "    else:\n",
    "        auc = metrics.roc_auc_score(y_true=to_categorical(y_true, num_classes=2),\n",
    "                                    y_score=to_categorical(y_pred, num_classes=2))\n",
    "    # Printing results\n",
    "    headers.append('-'), headers.append('micro-Avg'), headers.append('macro-Avg')\n",
    "    print(tabulate(my_data, headers=headers))\n",
    "\n",
    "    print('------------------------\\nAUC', round(auc, 4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print('Plot confusion_matrix:')\n",
    "    plot_confusion_matrix(y_true, y_pred, numfold, classes, dir_out=paths)\n",
    "\n",
    "    # Coeficiente de cohen Kappa\n",
    "    print('Coeficiente de cohen Kappa:')\n",
    "    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    print(kappa)\n",
    "\n",
    "    # print('Plot curve ROC:')\n",
    "    # curve_ROC(len(classes),y_true,y_pred,numfold,dir_out=paths['folder_to_saveMetrics'])\n",
    "    return micro_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n",
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n",
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n",
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n",
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n",
      "\n",
      "This model is 0.00 percent confident that CT scan is NORMAL\n",
      "This model is 100.00 percent confident that CT scan is HIC\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "                   Sano    HIC  -      micro-Avg    macro-Avg\n",
      "---------------  ------  -----  ---  -----------  -----------\n",
      "Sensitivity           1      1  ---            1            1\n",
      "Specificity           1      1  ---            1            1\n",
      "PPV (precision)       1      1  ---            1            1\n",
      "NPV                   1      1  ---            1            1\n",
      "F1-score              1      1  ---            1            1\n",
      "Accuracy              1      1  ---            1            1\n",
      "------------------------\n",
      "AUC 1.0\n",
      "Plot confusion_matrix:\n",
      "Confusion matrix, without normalization\n",
      "[[26  0]\n",
      " [ 0 16]]\n",
      "Coeficiente de cohen Kappa:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load best weights.\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "for iifold in range(0,6):\n",
    "    model=load_model('/srv/data/HIC/code/resultados/DICOM/DICOM_topGMP_batch2_optAdadelta_lr0.35_kfold'+str(iifold)+'.h5')\n",
    "    #model.load_weights(\"/srv/data/HIC/code/resultados/DICOM/DICOM_topGMP_batch2_optAdadelta_lr0.01_kfold1.h5\")\n",
    "    x_test=[]\n",
    "    CSV_TEST = pd.read_csv('/srv/data/HIC/datos/csv/TEST.csv',sep=',')\n",
    "    test_dataset=[]\n",
    "    for i in CSV_TEST['name']:\n",
    "        much_data= process_scan(i)  \n",
    "        test_dataset.append(much_data)         \n",
    "    x_test=np.array(test_dataset)\n",
    "    x_test = np.expand_dims(x_test, axis=4)\n",
    "    y_test=CSV_TEST['label']\n",
    "\n",
    "    y_pred = model.predict(x_test)  \n",
    "    #print(y_pred.round())\n",
    "    y_pred=y_pred.round()\n",
    "    prediction = y_pred[0]\n",
    "    scores = [1 - prediction[0], prediction[0]]\n",
    "    print()\n",
    "    class_names = [\"NORMAL\", \"HIC\"]\n",
    "    for score, name in zip(scores, class_names):\n",
    "        print(\n",
    "            \"This model is %.2f percent confident that CT scan is %s\"\n",
    "            % ((100 * score), name)\n",
    "        )\n",
    "\n",
    "\n",
    "    classes = {'0': 'Sano', '1': 'HIC'}\n",
    "    plot_confusion_matrix(y_test, y_pred, iifold, classes)\n",
    "    report_classification_results(y_test, y_pred, classes, iifold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
