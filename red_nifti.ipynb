{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%pip install --upgrade pip\n",
    "%pip install --upgrade gast\n",
    "%pip install --upgrade tensorflow-gpu\n",
    "%pip install --upgrade tensorboard'''\n",
    "\n",
    "'''%pip install gast==0.3.3\n",
    "%pip install tensorflow==2.2.0\n",
    "%pip install opencv-python-headless'''\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta, SGD\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import nibabel as nib    #para leer los nifti\n",
    "from scipy import ndimage   #Multidimensional image processing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# \n",
    "#...............PREPROCESADO.................\n",
    "#############################################\n",
    "\n",
    "def read_file(filepath):   #Aquí solo define la función. \n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    #print(filepath)\n",
    "    much_data = np.load(filepath)\n",
    "    \n",
    "    #print(much_data.shape)\n",
    "    \n",
    "    return much_data\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 45\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def resize_volume_black(img):    \n",
    "    n,m,l=img.shape\n",
    "    #print('img',img.shape)\n",
    "    img_p=list(img)\n",
    "\n",
    "    matriz_float = np.zeros((m,l))\n",
    "    #print('matriz_float',matriz_float.shape)\n",
    "    \n",
    "    matriz_float = np.float32(matriz_float)\n",
    "    for ind in range(n,45):\n",
    "        img_p.append(matriz_float)\n",
    "    img_p=np.array(img_p)\n",
    "    #print('img_nuevo',img_p.shape)\n",
    "    \n",
    "    \n",
    "    n,m,l=img_p.shape\n",
    "    img_data=np.zeros([128,128,n])\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        img_data[:,:,i]=cv2.resize(img_p[i,:,:],(128,128))\n",
    "    '''hist(img_data)\n",
    "    sample_stack(img_data, rows=6, cols=6, start_with=0, show_every=1)'''\n",
    "    #print(img_data.shape)\n",
    "    \n",
    "    return img_data\n",
    "\n",
    "#COMPENDIO DE LAS FUNCIONES DEFINIDAS ARRIBA:\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    #print(path)\n",
    "    # Read scan\n",
    "    volume = read_file(path)    #no importan tanto el nombre (path o filepath) sino el ORDEN!!!\n",
    "    '''hist(volume)\n",
    "    sample_stack(volume, rows=4, cols=4, start_with=0, show_every=1)'''\n",
    "    #print(volume.shape)\n",
    "   \n",
    "    return volume\n",
    "\n",
    "# VISUALIZAR CORTES\n",
    "def sample_stack(stack, rows=5, cols=5, start_with=0, show_every=1):\n",
    "    fig,ax = plt.subplots(rows,cols,figsize=[12,12])\n",
    "    for i in range(rows*cols):\n",
    "        ind = start_with + i*show_every\n",
    "        ax[int(i/rows),int(i % rows)].set_title('slice %d' % ind)\n",
    "        ax[int(i/rows),int(i % rows)].imshow(stack[:,:,ind],cmap='gray')\n",
    "        ax[int(i/rows),int(i % rows)].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# VISUALIZAR CORTES\n",
    "def hist(stack):\n",
    "    fig= plt.hist(stack.flatten(), bins=80)\n",
    "    plt.xlabel(\"Hounsfield Units (HU)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#....INICIALIZAR PARÁMETROS PARA EL ENTRENAMIENTO..............\n",
    "###############################################################\n",
    "Kfold=[1,2,3,4,5]\n",
    "batchs=[2] \n",
    "epochs = 20\n",
    "lr_schedule=[0.6,0.5,0.4, 0.3, 0.2]\n",
    "optimizer_model=['Adadelta'] # 'SGD', 'Adam', 'SGD'\n",
    "top=['GMP'] #,'GAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] FOLD 1\n",
      "\n",
      "[INFO] LEYENDO CSV\n",
      "Number of samples in train and validation are 128 and 32.\n",
      "[INFO] BATCH 2\n",
      "[INFO] OPT Adadelta\n",
      "[INFO] LR 0.6\n",
      "[INFO] topmodel GMP\n",
      "[INFO] OPTIMIZADOR:  Adadelta\n",
      "[INFO] ENTRENAMIENTO\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 5 dimensions, but got array with shape (128, 128, 128, 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f570bb6096d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                         )\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 5 dimensions, but got array with shape (128, 128, 128, 45)"
     ]
    }
   ],
   "source": [
    "############################################# \n",
    "#.............ARQUITECTURA...................\n",
    "#############################################\n",
    "def get_model(width=128, height=128, depth=64, top='GMP'):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if top is 'GMP':\n",
    "        x = layers.GlobalMaxPooling3D()(x)\n",
    "        x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "'''def get_model_anna(width=128, height=128, depth=45, top='GAP'):\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "    #initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    x = layers.Conv3D(filters=16, kernel_size=3,  activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=32, kernel_size=3,  activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=128, kernel_size=3,  activation=\"relu\")(x)\n",
    "\n",
    "    if top is 'GMP':\n",
    "        x = layers.GlobalMaxPooling3D()(x)\n",
    "        x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        x = layers.Dense(units=512,  activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model'''\n",
    "def get_model_anna(width=128, height=128, depth=45, top='GAP'):\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "            #initializer = tf.kehist(stack)as.initializers.GlorotNormal()\n",
    "    \n",
    "    \n",
    "    x = layers.Conv3D(filters=16, kernel_size=3,  activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=32, kernel_size=3,  activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=3,  activation=\"relu\")(x)\n",
    "\n",
    "    \n",
    "    if top is 'GMP':\n",
    "        x = layers.GlobalMaxPooling3D()(x)\n",
    "        '''x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)'''\n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        '''x = layers.Dense(units=512,  activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)'''\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "############################################# \n",
    "#.............OPTIMIZADOR...................\n",
    "#############################################\n",
    "def _get_optimizer(optimizer_model, lr_n):\n",
    "    print(\"[INFO] OPTIMIZADOR: \", optimizer_model)\n",
    "    # Initiate the optimizer.\n",
    "    if optimizer_model == 'Adam':\n",
    "        opt = Adam(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'Adagrad':\n",
    "        opt = Adagrad(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'Adadelta':\n",
    "        opt = Adadelta(lr_n)\n",
    "\n",
    "    elif optimizer_model == 'SGD':\n",
    "        opt = SGD(lr_n)\n",
    "    else:\n",
    "        raise ValueError('Optimizer not implemented yet!')\n",
    "    return opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#___________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "############################################# \n",
    "#.............ENTRENAMIENTO..................\n",
    "#############################################\n",
    "\n",
    "for ifold in Kfold:\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_val=[]\n",
    "    y_val=[]\n",
    "    CSV_TRAIN=[]\n",
    "    CSV_VAL=[]\n",
    "    \n",
    "    print('[INFO] FOLD',str(ifold))\n",
    "    print('\\n[INFO] LEYENDO CSV')\n",
    "    if ifold is not 0:\n",
    "        CSV_TRAIN = pd.read_csv('/srv/data/HIC/datos/csv/NIFTI_TRAIN'+str(ifold)+'.csv',sep=',')\n",
    "        CSV_VAL = pd.read_csv('/srv/data/HIC/datos/csv/NIFTI_VAL'+str(ifold)+'.csv',sep=',')\n",
    "    else:\n",
    "        CSV_TRAIN = pd.read_csv('/srv/data/HIC/datos/csv/NIFTI_TRAIN.csv',sep=',')\n",
    "        CSV_VAL = pd.read_csv('/srv/data/HIC/datos/csv/NIFTI_TEST.csv',sep=',')\n",
    "        \n",
    "    x_train = [process_scan(path) for path in CSV_TRAIN['name']] #es un array 4D (100 (nº de TC) x 128 x 128 x 64)\n",
    "    y_train=CSV_TRAIN['label']\n",
    "\n",
    "    x_val = [process_scan(path) for path in CSV_VAL['name']]\n",
    "    y_val=CSV_VAL['label']\n",
    "    \n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_val = np.asarray(x_val)\n",
    "    y_val = np.asarray(y_val)\n",
    "\n",
    "    print(\n",
    "        \"Number of samples in train and validation are %d and %d.\" \n",
    "        % (x_train.shape[0], x_val.shape[0])\n",
    "    )\n",
    "    '''# Define data loaders.\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    print(train_loader)'''\n",
    "    \n",
    "    for ibatch in batchs:\n",
    "        batch_size = ibatch\n",
    "        print('[INFO] BATCH',str(ibatch))\n",
    "        \n",
    "        '''# Augment the on the fly during training.\n",
    "        train_dataset = (\n",
    "            train_loader.shuffle(len(x_train))\n",
    "            .map(train_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "        # Only rescale.\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(x_val))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )'''\n",
    "    \n",
    "    \n",
    "        for iopt in optimizer_model:\n",
    "            print('[INFO] OPT',str(iopt))\n",
    "            \n",
    "            for ilr in lr_schedule: \n",
    "                for itop in top:\n",
    "                    print('[INFO] LR',str(ilr))\n",
    "                    print('[INFO] topmodel',str(itop))\n",
    "\n",
    "                    # Build model.\n",
    "                    model = get_model_anna(width=128, height=128, depth=45, top=itop)\n",
    "\n",
    "                    # Compile model.\n",
    "                    '''initial_learning_rate = 0.0001\n",
    "                    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "                    )'''\n",
    "                    opt = _get_optimizer(iopt, ilr)\n",
    "                    model.compile(\n",
    "                        loss=\"binary_crossentropy\",\n",
    "                        optimizer=opt,\n",
    "                        metrics=[\"acc\"],\n",
    "                    )\n",
    "\n",
    "                    # Define callbacks.\n",
    "                    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "                        \"/srv/data/HIC/code/resultados/NIFTI/MODELO_top\"+str(itop)+\"batch\"+str(ibatch)+\"_opt\"+str(iopt)+\"_lr\"+str(ilr)+\"_kfold\"+str(ifold)+\".h5\", save_best_only=True\n",
    "                    )\n",
    "                    early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "                    \n",
    "                    '''class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                                                     , np.unique(y_train)\n",
    "                                                                     , y_train)'''\n",
    "                    # Train the model, doing validation at the end of each epoch\n",
    "                    '''for ind in range(0,15):\n",
    "                        sample_stack(x_train[ind,:,:,:] , rows=6, cols=6, start_with=0, show_every=1)\n",
    "                        hist(x_train[ind,:,:,:])'''\n",
    "                    print('[INFO] ENTRENAMIENTO')\n",
    "                    model.fit(\n",
    "                        x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=ibatch,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "                        )\n",
    "\n",
    "                    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "                    ax = ax.ravel()\n",
    "                    for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "                        ax[i].plot(model.history.history[metric])\n",
    "                        ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "                        ax[i].set_title(\"Model {}\".format(metric))\n",
    "                        ax[i].set_xlabel(\"epochs\")\n",
    "                        ax[i].set_ylabel(metric)\n",
    "                        ax[i].legend([\"train\", \"val\"])\n",
    "                        plt.savefig(\"/srv/data/HIC/code/resultados/NIFTI/LOSS_ACC_top\"+str(itop)+\"batch\"+str(ibatch)+\"_opt\"+str(iopt)+\"_lr\"+str(ilr)+\"_kfold\"+str(ifold)+\".png\")\n",
    "                    # Clean GPU memory\n",
    "                    del model\n",
    "                    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from keras.utils import to_categorical\n",
    "from tabulate import tabulate\n",
    "def plot_confusion_matrix(y_true, y_pred, numfold, classes, normalize=False, title=None, cmap=plt.cm.Blues, dir_out='/srv/data/HIC/code/resultados/NIFTI/'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = [classes['0'], classes['1'], classes['2'], classes['3']]\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax.figure.savefig(dir_out + '/kfold' + str(numfold))\n",
    "    plt.close()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def report_classification_results(y_true, y_pred, classes, numfold, paths='/srv/data/HIC/code/resultados/NIFTI/'):\n",
    "    # Extract confusion matrix\n",
    "    conf_mat = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute metrics\n",
    "    headers = []\n",
    "    g_TP, g_TN, g_FN, g_FP = [], [], [], []\n",
    "    ss, ee, pp, nn, ff, aa = [], [], [], [], [], []\n",
    "    S, E, PPV, NPV, F, ACC = ['Sensitivity'], ['Specificity'], ['PPV (precision)'], ['NPV'], ['F1-score'], ['Accuracy']\n",
    "    for i in range(len(classes)):\n",
    "        headers.append(classes[str(i)])\n",
    "\n",
    "        # Extract indicators per class\n",
    "        TP = conf_mat[i, i]\n",
    "        TN = sum(sum(conf_mat)) - sum(conf_mat[i, :]) - sum(conf_mat[:, i]) + TP\n",
    "        FN = sum(conf_mat[i, :]) - conf_mat[i, i]\n",
    "        FP = sum(conf_mat[:, i]) - conf_mat[i, i]\n",
    "\n",
    "        # Extract metrics per class\n",
    "        sen = TP / (TP + FN)\n",
    "        spe = TN / (TN + FP)\n",
    "        ppv = TP / (TP + FP)\n",
    "        npv = TN / (TN + FN)\n",
    "        f1_s = 2 * ppv * sen / (ppv + sen)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        # Create table for printing\n",
    "        S.append(str(round(sen, 4)))\n",
    "        E.append(str(round(spe, 4)))\n",
    "        PPV.append(str(round(ppv, 4)))\n",
    "        NPV.append(str(round(npv, 4)))\n",
    "        F.append(str(round(f1_s, 4)))\n",
    "        ACC.append(str(round(acc, 4)))\n",
    "\n",
    "        # Global indicators\n",
    "        g_TP.append(TP), g_TN.append(TN), g_FP.append(FP), g_FN.append(FN)\n",
    "        # Global metrics\n",
    "        ss.append(sen), ee.append(spe), pp.append(ppv), nn.append(npv), ff.append(f1_s), aa.append(acc)\n",
    "\n",
    "    # Extract micro values\n",
    "    micro_S = round(sum(g_TP) / (sum(g_TP) + sum(g_FN)), 4)\n",
    "    micro_E = round(sum(g_TN) / (sum(g_TN) + sum(g_FP)), 4)\n",
    "    micro_PPV = round(sum(g_TP) / (sum(g_TP) + sum(g_FP)), 4)\n",
    "    micro_NPV = round(sum(g_TN) / (sum(g_TN) + sum(g_FN)), 4)\n",
    "    micro_F1 = round(2 * micro_S * micro_PPV / (micro_S + micro_PPV), 4)\n",
    "    micro_Acc = round((sum(g_TP) + sum(g_TN)) / (sum(g_TP) + sum(g_TN) + sum(g_FP) + sum(g_FN)), 4)\n",
    "    # Extract macro values\n",
    "    macro_S = round(sum(ss) / len(ss), 4)\n",
    "    macro_E = round(sum(ee) / len(ee), 4)\n",
    "    macro_PPV = round(sum(pp) / len(pp), 4)\n",
    "    macro_NPV = round(sum(nn) / len(nn), 4)\n",
    "    macro_F1 = round(sum(ff) / len(ff), 4)\n",
    "    macro_Acc = round(sum(aa) / len(aa), 4)\n",
    "\n",
    "    # Construct the table\n",
    "    S.append('---'), E.append('---'), PPV.append('---'), NPV.append('---'), F.append('---'), ACC.append('---')\n",
    "    S.append(micro_S), E.append(micro_E), PPV.append(micro_PPV), NPV.append(micro_NPV), F.append(micro_F1), ACC.append(\n",
    "        micro_Acc)\n",
    "    S.append(macro_S), E.append(macro_E), PPV.append(macro_PPV), NPV.append(macro_NPV), F.append(macro_F1), ACC.append(\n",
    "        macro_Acc)\n",
    "\n",
    "    # Define table\n",
    "    my_data = [tuple(S), tuple(E), tuple(PPV), tuple(NPV), tuple(F), tuple(ACC)]\n",
    "    df = pd.DataFrame(my_data)\n",
    "    df.to_csv(paths + '/metrics_kfold' + str(numfold) + '.csv')\n",
    "\n",
    "    # MODEL metrics\n",
    "    if len(classes) > 2:\n",
    "        auc = metrics.roc_auc_score(y_true=to_categorical(y_true, num_classes=len(classes)),\n",
    "                                    y_score=to_categorical(y_pred, num_classes=len(classes)), multi_class='ovr')\n",
    "    else:\n",
    "        auc = metrics.roc_auc_score(y_true=to_categorical(y_true, num_classes=2),\n",
    "                                    y_score=to_categorical(y_pred, num_classes=2))\n",
    "    # Printing results\n",
    "    headers.append('-'), headers.append('micro-Avg'), headers.append('macro-Avg')\n",
    "    print(tabulate(my_data, headers=headers))\n",
    "\n",
    "    print('------------------------\\nAUC', round(auc, 4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print('Plot confusion_matrix:')\n",
    "    plot_confusion_matrix(y_true, y_pred, numfold, classes, dir_out=paths)\n",
    "\n",
    "    # Coeficiente de cohen Kappa\n",
    "    print('Coeficiente de cohen Kappa:')\n",
    "    kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    print(kappa)\n",
    "\n",
    "    # print('Plot curve ROC:')\n",
    "    # curve_ROC(len(classes),y_true,y_pred,numfold,dir_out=paths['folder_to_saveMetrics'])\n",
    "    return micro_S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load best weights.\n",
    "from keras.engine.saving import load_model, save_model\n",
    "from sklearn import metrics\n",
    "iifold=0\n",
    "model=load_model('/srv/data/HIC/code/resultados/NIFTI/MODELO_topGMPbatch2_optAdadelta_lr0.7_kfold'+str(iifold)+'.h5')\n",
    "#model.load_weights(\"/srv/data/HIC/code/resultados/DICOM/DICOM_topGMP_batch2_optAdadelta_lr0.01_kfold1.h5\")\n",
    "x_test=[]\n",
    "CSV_TEST = pd.read_csv('/srv/data/HIC/datos/csv/NIFTI_TEST.csv',sep=',')\n",
    "test_dataset=[]\n",
    "for i in CSV_TEST['name']:\n",
    "    much_data= process_scan(i)  \n",
    "    test_dataset.append(much_data)         \n",
    "x_test=np.array(test_dataset)\n",
    "y_test=CSV_TEST['label']\n",
    "\n",
    "y_pred = model.predict(x_test)  \n",
    "#print(y_pred.round())\n",
    "y_pred=y_pred.round()\n",
    "prediction = y_pred[0]\n",
    "scores = [1 - prediction[0], prediction[0]]\n",
    "print()\n",
    "class_names = [\"NORMAL\", \"HIC\"]\n",
    "for score, name in zip(scores, class_names):\n",
    "    print(\n",
    "        \"This model is %.2f percent confident that CT scan is %s\"\n",
    "        % ((100 * score), name)\n",
    "    )\n",
    "    \n",
    "\n",
    "classes = {'0': 'Sano', '1': 'HIC'}\n",
    "plot_confusion_matrix(y_test, y_pred, iifold, classes)\n",
    "report_classification_results(y_test, y_pred, classes, iifold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
